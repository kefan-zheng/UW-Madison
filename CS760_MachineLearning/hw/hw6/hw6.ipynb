{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da03d26-2805-4a61-a29f-e4cb81ad4cbc",
   "metadata": {},
   "source": [
    "#### Name: Kefan Zheng\n",
    "#### StudentId: 9086175008\n",
    "#### Email: kzheng58@wisc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e06cea-1f17-42e5-ad90-6dfb24ff7efa",
   "metadata": {},
   "source": [
    "# Problem 6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb04b8-9630-443d-9cc7-94780cd52bf1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p^{*}}_{ML} = \\mathop{\\arg\\max}_{p} \\;\\; \\mathbb{P}(X \\mid p) = \\mathop{\\arg\\max}_{p} \\;\\; {p}^{1^TX}{(1-p)}^{N-1^TX}\n",
    "$$\n",
    "Take log:  \n",
    "$$\n",
    "\\hat{p^{*}}_{ML} = \\mathop{\\arg\\max}_{p} \\;\\; \\log(\\mathbb{P}(X \\mid p)) = \\mathop{\\arg\\max}_{p} \\;\\; 1^TX\\log(p) + (N-1^TX)\\log(1-p)\n",
    "$$\n",
    "Take the derivative:  \n",
    "$$\n",
    "\\frac{\\partial \\log(\\mathbb{P}(X \\mid p))}{\\partial p} = \\frac{1^TX}{p} - \\frac{N-1^TX}{1-p}\n",
    "$$\n",
    "Set to zero:\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\log(\\mathbb{P}(X \\mid p))}{\\partial p} = 0 \\\\\n",
    "\\frac{1^TX}{p} - \\frac{N-1^TX}{1-p} = 0 \\\\\n",
    "\\frac{1^TX(1-p) - p(N-1^TX))}{p(1-p)} = 0 \\\\\n",
    "\\frac{1^TX-pN}{p(1-p)} = 0 \\\\\n",
    "p = \\frac{1^TX}{N}\n",
    "\\end{align}\n",
    "So\n",
    "$$\n",
    "\\hat{p^{*}}_{ML} = \\frac{1^TX}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509277b-cbae-4df3-b845-58e8a6f8a854",
   "metadata": {},
   "source": [
    "# Problem 6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530db628-60f0-45f7-bf97-3ed34ac299ab",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p^{*}}_{MAP} := \\mathop{\\arg\\max}_{p} \\;\\; \\mathbb{P}(p \\mid X)\n",
    "$$\n",
    "Rewrite using Bayes Rule:  \n",
    "$$\n",
    "\\hat{p^{*}}_{MAP} = \\mathop{\\arg\\max}_{p} \\;\\; \\frac{\\mathbb{P}(X \\mid p)\\mathbb{P}(p)}{\\mathbb{P}(X)} = \\mathop{\\arg\\max}_{p} \\;\\; \\mathbb{P}(X \\mid p)\\mathbb{P}(p)\n",
    "$$\n",
    "Model the prior of $\\mathbb{P}(p)$ as $Beta(\\alpha, \\beta)$ with parameters $\\alpha \\gt \\beta \\gt 1$:  \n",
    "$$\n",
    "\\mathbb{P}(p) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha - 1}(1 - p)^{\\beta - 1}\n",
    "$$\n",
    "So\n",
    "\\begin{align}\n",
    "\\hat{p^{*}}_{MAP} &= \\mathop{\\arg\\max}_{p} \\;\\; \\mathbb{P}(p \\mid X) \\\\\n",
    "&= \\mathop{\\arg\\max}_{p} \\;\\; \\mathbb{P}(X \\mid p)\\mathbb{P}(p) \\\\\n",
    "&= \\mathop{\\arg\\max}_{p} \\;\\; \\Big({p}^{1^TX}{(1-p)}^{N-1^TX} \\Big)\\bigg(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha - 1}(1 - p)^{\\beta - 1}\\bigg) \\\\\n",
    "\\end{align}\n",
    "Take log:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{p^{*}}_{MAP} &= \\mathop{\\arg\\max}_{p} \\;\\; \\log\\bigg( \\mathbb{P}(X \\mid p)\\mathbb{P}(p) \\bigg) \\\\\n",
    "&= \\mathop{\\arg\\max}_{p} \\;\\; \\log\\Bigg(\\Big({p}^{1^TX}{(1-p)}^{N-1^TX} \\Big)\\bigg(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha - 1}(1 - p)^{\\beta - 1}\\bigg) \\Bigg) \\\\\n",
    "&= \\mathop{\\arg\\max}_{p} \\;\\; \\log\\bigg({p}^{1^TX}{(1-p)}^{N-1^TX}\\bigg) + \\log\\bigg(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha - 1}(1 - p)^{\\beta - 1}\\bigg) \\\\\n",
    "&= \\mathop{\\arg\\max}_{p} \\;\\; 1^TX\\log(p) + (N-1^TX)\\log(1-p) + \\log(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}) + (\\alpha-1)\\log(p) + (\\beta-1)\\log(1-p)\n",
    "\\end{align}\n",
    "\n",
    "Take the derivative:  \n",
    "\\begin{align}\n",
    "\\frac{\\partial \\log(\\mathbb{P}(X \\mid p)\\mathbb{P}(p))}{\\partial p} &= \\frac{1^TX}{p} - \\frac{N-1^TX}{1-p} + 0 + \\frac{\\alpha-1}{p} - \\frac{\\beta-1}{1-p} \\\\\n",
    "&= \\frac{1^TX+\\alpha-1}{p} - \\frac{N-1^TX+\\beta-1}{1-p} \\\\\n",
    "&= \\frac{1^TX+\\alpha-1-p(\\alpha+N+\\beta-2)}{p(1-p)}\n",
    "\\end{align}\n",
    "\n",
    "Set to zero:  \n",
    "\\begin{align}\n",
    "\\frac{\\partial \\log(\\mathbb{P}(X \\mid p)\\mathbb{P}(p))}{\\partial p} &= 0 \\\\\n",
    "\\frac{1^TX+\\alpha-1-p(\\alpha+N+\\beta-2)}{p(1-p)} &= 0 \\\\\n",
    "p(\\alpha+N+\\beta-2) &= 1^TX+\\alpha-1 \\\\\n",
    "p &= \\frac{1^TX+\\alpha-1}{\\alpha+N+\\beta-2}\n",
    "\\end{align}\n",
    "So\n",
    "$$\n",
    "\\hat{p^{*}}_{MAP} = \\frac{1^TX+\\alpha-1}{\\alpha+N+\\beta-2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f05f85e-e4bb-496e-9853-feaa29d74c40",
   "metadata": {},
   "source": [
    "# Problem 6.3 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63b189fc-7dd6-45b3-8a4b-4bb29ad74c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples required:  58\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mle_estimate(X):\n",
    "    N = len(X)\n",
    "    ones_vector = np.ones(N)\n",
    "    return (ones_vector @ X) / N\n",
    "\n",
    "def map_estimate(X, alpha, beta):\n",
    "    N = len(X)\n",
    "    ones_vector = np.ones(N)\n",
    "    return (ones_vector @ X + alpha -1) / (N + beta + alpha - 2)\n",
    "\n",
    "p_star = 0.99\n",
    "p_hat = float(\"inf\")\n",
    "N = 0\n",
    "while abs(p_star - p_hat) >= 0.01:\n",
    "    N += 1\n",
    "    X = np.random.binomial(1, p_star, N)\n",
    "    p_hat = mle_estimate(X)\n",
    "\n",
    "print(\"Number of samples required: \", N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f8e1c-7141-4440-9d7f-b6cbf966bd3c",
   "metadata": {},
   "source": [
    "# Problem 6.3 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b6453c-cf19-4a46-b13b-6e745837070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples required:  44\n"
     ]
    }
   ],
   "source": [
    "alpha = 7\n",
    "beta = 2\n",
    "p_star = 0.99\n",
    "p_hat = float(\"inf\")\n",
    "N = 0\n",
    "while abs(p_star - p_hat) >= 0.01:\n",
    "    N += 1\n",
    "    X = np.random.binomial(1, p_star, N)\n",
    "    p_hat = map_estimate(X, alpha, beta)\n",
    "\n",
    "print(\"Number of samples required: \", N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec75eb4-af00-4f39-9273-6eac52d5fcd9",
   "metadata": {},
   "source": [
    "# Problem 6.4 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f68755-9c79-4679-b38a-824e20691fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples required:  58\n"
     ]
    }
   ],
   "source": [
    "p_star = 0.01\n",
    "p_hat = float(\"inf\")\n",
    "N = 0\n",
    "while abs(p_star - p_hat) >= 0.01:\n",
    "    N += 1\n",
    "    X = np.random.binomial(1, p_star, N)\n",
    "    p_hat = mle_estimate(X)\n",
    "\n",
    "print(\"Number of samples required: \", N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43ef44-a948-4f78-9841-9576dda7f041",
   "metadata": {},
   "source": [
    "# Problem 6.4 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ec9f8e-05c7-45ac-a12c-6d82eb0ba1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples required:  324\n"
     ]
    }
   ],
   "source": [
    "alpha = 7\n",
    "beta = 2\n",
    "p_star = 0.01\n",
    "p_hat = float(\"inf\")\n",
    "N = 0\n",
    "while abs(p_star - p_hat) >= 0.01:\n",
    "    N += 1\n",
    "    X = np.random.binomial(1, p_star, N)\n",
    "    p_hat = map_estimate(X, alpha, beta)\n",
    "\n",
    "print(\"Number of samples required: \", N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b6bfd-e70e-445e-9f80-dbf9a3df7956",
   "metadata": {},
   "source": [
    "# Problem 6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcafea0-fa3f-4632-ba59-bdb7c390bef5",
   "metadata": {},
   "source": [
    "It depends on the specific scenario! When the prior information is certain, I prefer **MAP**, because MAP is more accurate than MLE when the number of samples is the same; And when there is a lack of prior information, I prefer **MLE** because MLE only focuses on the observed data and is applicable to most cases.  \n",
    "1. **MLE Advantages:**\n",
    "   1. Solid math foundation and easy to understand.\n",
    "   2. Wider application scope because it only focuses on observation samples and prior information is not necessary. \n",
    "2. **MLE Disadvantages:**\n",
    "   1. Prior information is not used if it exists.\n",
    "   2. Easy to overfit when the number of samples is small.   \n",
    "3. **MAP Advantages:**\n",
    "   1. Prior information is used to make estimation more accurate.\n",
    "   2. Performance is better on small sample data with prior information.\n",
    "   3. Solid math foundation     \n",
    "4. **MAP Disadvantages:**\n",
    "   1. Not all situations have prior information.\n",
    "   2. Improper prior information can mislead results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a395e3-8a74-4f19-9bf0-32bde52d5ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
